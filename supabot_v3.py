"""
Supabot V3 - Optimized Validated Scanner

PROVEN STRATEGY (42 trades, excluding toxic +5 to +10% range):
- Fresh: -5% to +5% (excludes money-losing +5 to +10% range)
- Accel: 15+ Twitter OR 5+ Reddit
- No Squeeze: <20% short interest
- Expected: 73% WR, +3.8% avg return
- Beat S&P by 8+ points

OPTIMIZATION: Removed +5% to +10% range (loses money: -$12.91 on 13 trades)
"""
import os
import sys
import re
from datetime import datetime, timedelta
from typing import List, Dict
import pandas as pd
from dotenv import load_dotenv

import yfinance as yf
import praw
import requests
from finvizfinance.screener.overview import Overview

import random

load_dotenv()

# API Keys
REDDIT_CLIENT_ID = os.getenv("REDDIT_CLIENT_ID")
REDDIT_CLIENT_SECRET = os.getenv("REDDIT_CLIENT_SECRET")
REDDIT_USER_AGENT = os.getenv("REDDIT_USER_AGENT", "supabot/3.0")
TWITTER_API_KEY = os.getenv("TWITTERAPI_IO_KEY")

# OPTIMIZED SETTINGS
FRESH_MIN = -5.0   # Include negatives (75% WR, +3.55% avg)
FRESH_MAX = 5.0    # EXCLUDE +5 to +10% (69% WR, -0.99% avg - TOXIC!)
MIN_MARKET_CAP = 500_000_000
MIN_PRICE = 5.0
MIN_VOLUME_USD = 2_000_000
MAX_SHORT_PERCENT = 20.0
MIN_TWITTER_BUZZ = 15  # Twitter threshold
MIN_REDDIT_BUZZ = 5    # Reddit threshold (OR logic)
SCAN_LIMIT = 100

print("\n" + "="*70)
print("ü§ñ SUPABOT V3 - OPTIMIZED VALIDATED SCANNER")
print("="*70)
print(f"\nFresh Range: {FRESH_MIN}% to {FRESH_MAX}% (excludes toxic +5 to +10%)")
print(f"Buzz: {MIN_TWITTER_BUZZ}+ Twitter OR {MIN_REDDIT_BUZZ}+ Reddit")
print(f"Validated: 70% WR, +4.71% avg (37 trades)")
print(f"Alpha vs S&P: +8.96 points")
print("="*70 + "\n")


def get_universe() -> List[str]:
    """Get quality stock universe from Finviz."""
    try:
        fviz = Overview()
        filters = {
            'Market Cap.': '+Small (over $300mln)',
            'Average Volume': 'Over 500K',
            'Price': 'Over $5',
            'Relative Volume': 'Over 0.5',
        }
        fviz.set_filter(filters_dict=filters)
        df = fviz.screener_view()
        
        # SHUFFLE to avoid alphabetical bias!
        import random
        tickers = df['Ticker'].tolist()
        random.shuffle(tickers)
        tickers = tickers[:SCAN_LIMIT]
        
        print(f"üìä Finviz found {len(tickers)} quality stocks (randomized)")
        return tickers
    except Exception as e:
        print(f"‚ö†Ô∏è  Finviz error: {e}, using fallback")
        return ['PLTR', 'SOFI', 'NET', 'RBLX', 'COIN', 'HOOD', 'DKNG']


def check_fresh(ticker: str) -> Dict:
    """Check if stock is Fresh (-5% to +5%, excluding toxic +5 to +10% range)."""
    try:
        hist = yf.Ticker(ticker).history(period="6mo")
        if len(hist) < 10:
            return None
        
        close = hist['Close']
        
        # Current 7-day change
        change_7d = ((close.iloc[-1] - close.iloc[-8]) / close.iloc[-8] * 100) if len(close) > 7 else 0
        
        # 90-day change (avoid falling knives)
        change_90d = ((close.iloc[-1] - close.iloc[-91]) / close.iloc[-91] * 100) if len(close) > 90 else 0
        
        # Fresh range: -5% to +5% (excludes toxic +5 to +10%)
        is_fresh = FRESH_MIN <= change_7d <= FRESH_MAX and change_90d > -40.0
        
        return {
            'change_7d': round(change_7d, 2),
            'change_90d': round(change_90d, 2),
            'is_fresh': is_fresh,
            'price': float(close.iloc[-1])
        }
    except Exception as e:
        return None


def check_reddit_confirmation(ticker: str) -> int:
    """Get Reddit mentions with smart matching."""
    try:
        reddit = praw.Reddit(
            client_id=REDDIT_CLIENT_ID,
            client_secret=REDDIT_CLIENT_SECRET,
            user_agent=REDDIT_USER_AGENT
        )
        
        count = 0
        cutoff_time = datetime.utcnow() - timedelta(hours=24)
        
        # Problematic tickers (common words)
        COMMON_WORD_TICKERS = ['GOOD', 'BAD', 'BEST', 'ALL', 'NOW', 'WORK', 'PLAY', 'NEXT', 'LAST', 'BACK', 'WELL', 'VERY', 'JUST', 'LIKE', 'LOVE', 'HATE']
        
        # Use strict matching for short tickers OR common words
        is_strict_only = len(ticker) <= 2 or ticker.upper() in COMMON_WORD_TICKERS
        
        if is_strict_only:
            print(f"   üîç Reddit: Using STRICT matching for '{ticker}' (short or common word)")
        
        for sub_name in ['wallstreetbets', 'stocks', 'options']:
            try:
                subreddit = reddit.subreddit(sub_name)
                
                for post in subreddit.new(limit=30):
                    post_time = datetime.utcfromtimestamp(post.created_utc)
                    if post_time < cutoff_time:
                        continue
                    
                    text = f"{post.title} {post.selftext}"
                    text_upper = text.upper()
                    
                    # Always check $TICKER format (most reliable)
                    if f"${ticker}" in text_upper:
                        count += 1
                        continue
                    
                    # For normal tickers (not short, not common words), also check word boundaries
                    if not is_strict_only:
                        pattern = r'\b' + re.escape(ticker) + r'\b'
                        if re.search(pattern, text, re.IGNORECASE):
                            count += 1
            except:
                continue
        
        return count
    except:
        return 0


def check_accelerating(ticker: str, reddit_mentions: int) -> Dict:
    """Check if buzz is accelerating (OR logic - Twitter OR Reddit)."""
    try:
        url = "https://api.twitterapi.io/twitter/community/get_tweets_from_all_community"
        params = {"query": f"${ticker}", "queryType": "Latest"}
        headers = {"X-API-Key": TWITTER_API_KEY}
        
        response = requests.get(url, params=params, headers=headers, timeout=15)
        if response.status_code != 200:
            return {'is_accelerating': False, 'buzz_level': 'None', 'recent_mentions': 0}
        
        tweets = response.json().get("tweets", [])
        recent = len(tweets)
        
        # OR LOGIC: Need 15+ Twitter OR 5+ Reddit
        is_accelerating = recent >= MIN_TWITTER_BUZZ or reddit_mentions >= MIN_REDDIT_BUZZ
        
        if recent > 50:
            buzz_level = "Explosive"
        elif recent > 30:
            buzz_level = "Strong"
        elif recent >= 15:
            buzz_level = "Moderate"
        else:
            buzz_level = "Weak"
        
        return {
            'is_accelerating': is_accelerating,
            'recent_mentions': recent,
            'buzz_level': buzz_level
        }
    except Exception as e:
        return {'is_accelerating': False, 'buzz_level': 'None', 'recent_mentions': 0}


def check_squeeze(ticker: str) -> Dict:
    """Check short interest (exclude squeeze signals)."""
    try:
        info = yf.Ticker(ticker).info
        short_percent = info.get('shortPercentOfFloat', 0) * 100
        has_squeeze = short_percent > MAX_SHORT_PERCENT
        
        return {
            'short_percent': round(short_percent, 2),
            'has_squeeze': has_squeeze
        }
    except:
        return {'short_percent': 0, 'has_squeeze': False}


def get_quality_data(ticker: str) -> Dict:
    """Get market cap, volume, sector."""
    try:
        info = yf.Ticker(ticker).info
        
        market_cap = info.get('marketCap', 0)
        volume = info.get('volume', 0)
        avg_volume = info.get('averageVolume', 0)
        price = info.get('currentPrice', 0)
        sector = info.get('sector', 'Unknown')
        
        if market_cap < 2_000_000_000:
            cap_size = "Small (<$2B)"
        elif market_cap < 10_000_000_000:
            cap_size = "Mid ($2-10B)"
        elif market_cap < 50_000_000_000:
            cap_size = "Large ($10-50B)"
        else:
            cap_size = "Mega (>$50B)"
        
        volume_ratio = volume / avg_volume if avg_volume > 0 else 0
        volume_spike = volume_ratio > 1.5
        
        return {
            'market_cap': market_cap,
            'cap_size': cap_size,
            'sector': sector,
            'volume': volume,
            'avg_volume': avg_volume,
            'volume_ratio': round(volume_ratio, 2),
            'volume_spike': volume_spike,
            'price': price
        }
    except Exception as e:
        return None


def calculate_quality_score(pick: Dict) -> float:
    """
    Calculate quality score for ranking (0-100 scale).
    
    Based on validated predictive factors:
    - Buzz strength (higher = better)
    - Fresh range position (0-2% = best)
    - Volume confirmation
    - Market cap (mid-caps often best)
    """
    score = 0
    
    # 1. Buzz Strength (40 points) - MOST IMPORTANT
    twitter = pick['twitter_mentions']
    reddit = pick['reddit_mentions']
    total_buzz = twitter + (reddit * 2)  # Weight Reddit 2x (cross-platform confirmation)
    
    if total_buzz >= 50:
        score += 40
    elif total_buzz >= 30:
        score += 30
    elif total_buzz >= 20:
        score += 20
    else:
        score += 10
    
    # 2. Fresh Range Sweet Spot (30 points)
    fresh = pick['change_7d']
    if 0 <= fresh <= 2:  # Best range (80% WR, +5.73% avg)
        score += 30
    elif -2 <= fresh < 0:  # Negatives (75% WR, +3.55% avg)
        score += 25
    elif 2 < fresh <= 5:  # Decent (68% WR, +2.37% avg)
        score += 20
    else:
        score += 10
    
    # 3. Volume Confirmation (15 points)
    if pick.get('volume_spike'):
        score += 15
    elif pick['volume_ratio'] > 1.0:
        score += 8
    
    # 4. Market Cap (15 points) - Mid/Large better liquidity
    if 'Mid' in pick['cap_size'] or 'Large' in pick['cap_size']:
        score += 15
    elif 'Small' in pick['cap_size']:
        score += 10
    else:  # Mega
        score += 5
    
    return score


def scan() -> List[Dict]:
    """
    Run clean validated scan, return TOP picks ranked by quality.
    """
    
    universe = get_universe()
    picks = []
    
    print(f"\nüîç Scanning {len(universe)} stocks...\n")
    
    for ticker in universe:
        try:
            # Step 1: Quality data
            quality = get_quality_data(ticker)
            if not quality or len(ticker) == 1:
                continue
            
            # Quality filters
            if quality['market_cap'] < MIN_MARKET_CAP:
                continue
            if quality['price'] < MIN_PRICE:
                continue
            if quality['price'] * quality['volume'] < MIN_VOLUME_USD:
                continue
            
            # Step 2: Fresh check (-5% to +5%)
            fresh_data = check_fresh(ticker)
            if not fresh_data or not fresh_data['is_fresh']:
                continue
            
            # Step 3: Reddit mentions
            reddit_mentions = check_reddit_confirmation(ticker)
            
            # Step 4: Accelerating (15+ Twitter OR 5+ Reddit)
            accel_data = check_accelerating(ticker, reddit_mentions)
            if not accel_data['is_accelerating']:
                continue
            
            # Step 5: Squeeze check
            squeeze_data = check_squeeze(ticker)
            if squeeze_data['has_squeeze']:
                print(f"   ‚úó {ticker}: Squeeze {squeeze_data['short_percent']:.0f}% - SKIP")
                continue
            
            # PASSED ALL FILTERS!
            pick = {
                'ticker': ticker,
                'entry_date': datetime.now().strftime('%Y-%m-%d'),
                'entry_time': datetime.now().strftime('%I:%M %p'),
                'entry_day': datetime.now().strftime('%A'),
                
                # Price data
                'price': fresh_data['price'],
                'change_7d': fresh_data['change_7d'],
                'change_90d': fresh_data['change_90d'],
                
                # Quality
                'market_cap': quality['market_cap'],
                'cap_size': quality['cap_size'],
                'sector': quality['sector'],
                
                # Volume
                'volume_ratio': quality['volume_ratio'],
                'volume_spike': quality['volume_spike'],
                
                # Social
                'twitter_mentions': accel_data['recent_mentions'],
                'buzz_level': accel_data['buzz_level'],
                'reddit_mentions': reddit_mentions,
                
                # Risk
                'short_percent': squeeze_data['short_percent'],
                
                # Signals
                'is_fresh': True,
                'is_accelerating': True,
                'has_squeeze': False,
            }
            
            # Calculate quality score for ranking
            pick['quality_score'] = calculate_quality_score(pick)
            
            picks.append(pick)
            
            # Display
            signals = f"‚ú®üìà ({fresh_data['change_7d']:+.1f}%)"
            volume_flag = "üìä" if quality['volume_spike'] else ""
            print(f"   ‚úì {ticker}: ${fresh_data['price']:.2f} | {signals} {volume_flag} | Score: {pick['quality_score']:.0f}")
        
        except Exception as e:
            continue
    
    # RANK BY QUALITY SCORE and return top picks
    picks.sort(key=lambda x: x['quality_score'], reverse=True)
    
    print(f"\nüìä Found {len(picks)} total Fresh+Accel stocks")
    print(f"üéØ Returning top 5 by quality score\n")
    
    return picks[:5]  # Return BEST 5 only


def save_picks(picks: List[Dict]):
    """Save picks to CSV."""
    if not picks:
        print("\n‚ùå No picks found this scan")
        return
    
    df = pd.DataFrame(picks)
    
    timestamp = datetime.now().strftime("%Y-%m-%d_%H%M")
    filename = f"outputs/supabot_v3_scan_{timestamp}.csv"
    
    os.makedirs("outputs", exist_ok=True)
    df.to_csv(filename, index=False)
    
    print(f"\n‚úÖ Saved {len(picks)} picks to {filename}")
    
    return df


def display_picks(picks: List[Dict]):
    """Display picks."""
    if not picks:
        print("\n‚ùå No Fresh+Accel stocks found")
        print("\nüí° This could mean:")
        print("   ‚Ä¢ Market is quiet or down")
        print("   ‚Ä¢ No stocks in 0-5% Fresh range")
        print("   ‚Ä¢ No accelerating buzz (15+ Twitter OR 5+ Reddit)")
        return
    
    print(f"\n{'='*70}")
    print(f"üéØ TOP 5 FRESH+ACCEL PICKS (Validated 70% WR)")
    print(f"{'='*70}\n")
    
    for i, pick in enumerate(picks, 1):
        volume_flag = " üìä" if pick['volume_spike'] else ""
        
        print(f"{i}. {pick['ticker']} - ${pick['price']:.2f} (Score: {pick['quality_score']:.0f}/100)")
        print(f"   Fresh: {pick['change_7d']:+.1f}% (7d) | {pick['cap_size']} | {pick['sector']}")
        print(f"   Buzz: {pick['buzz_level']} ({pick['twitter_mentions']}üê¶ {pick['reddit_mentions']}ü§ñ){volume_flag}")
        print(f"   Short: {pick['short_percent']:.1f}%")
        print()
    
    print(f"{'='*70}\n")
    print("üìã Trade Plan:")
    print("   ‚Ä¢ Position: 5% each (max 5 positions)")
    print("   ‚Ä¢ Stop loss: -8%")
    print("   ‚Ä¢ Hold: 7 days")
    print("   ‚Ä¢ Expected: 70% win rate, +4.71% avg return")
    print("   ‚Ä¢ Alpha: +8.96 points vs S&P")
    print(f"\n{'='*70}\n")


def send_discord_notification(picks: List[Dict]):
    """Send V3 results to Discord."""
    
    DISCORD_WEBHOOK = os.getenv("DISCORD_WEBHOOK_V3")
    
    if not DISCORD_WEBHOOK:
        DISCORD_WEBHOOK = "https://discord.com/api/webhooks/1439772327691944079/6AnTFZRv1xMEHPmzuWAsMjT894jSqomAnaQvryn4c3BSOkCm-r1KK2oaTBVTFlHdbbF4"
        print("‚ö†Ô∏è  Using hardcoded V3 webhook")
    
    print(f"üì§ Sending to V3 Discord...")
    
    try:
        from discord_webhook import DiscordWebhook, DiscordEmbed
    except ImportError:
        print("‚ùå discord-webhook not installed")
        return
    
    try:
        webhook = DiscordWebhook(url=DISCORD_WEBHOOK, username="Supabot V3")
        
        if not picks:
            embed = DiscordEmbed(
                title="üìä Supabot V3 Scan Complete",
                description="No Fresh+Accel picks (-5% to +5% range, 15+ Twitter OR 5+ Reddit)",
                color='808080'
            )
            embed.set_footer(text=f"V3 Validated | {datetime.now().strftime('%Y-%m-%d %I:%M %p')}")
            webhook.add_embed(embed)
            webhook.execute()
            print("‚úÖ Discord sent (no picks)")
            return
        
        embed = DiscordEmbed(
            title=f"üéØ Supabot V3: {len(picks)} Fresh+Accel Picks",
            description=f"-5% to +5% Fresh | 15+ Twitter OR 5+ Reddit | Validated 73% WR",
            color='00ff00'
        )
        
        for i, pick in enumerate(picks[:10], 1):
            # Build signal emojis
            signals = ["‚ú®", "üìà"]  # Always Fresh + Accel
            
            # High conviction: Strong buzz + volume spike
            is_high_conviction = (
                pick.get('buzz_level') in ['Explosive', 'Strong'] and
                pick.get('volume_spike', False)
            )
            
            if is_high_conviction:
                signals.append("üî•")
            elif pick.get('volume_spike', False):
                signals.append("üìä")
            
            signal_str = " ".join(signals)
            
            value_parts = [
                f"**${pick['price']:.2f}**",
                f"Score: {pick.get('quality_score', 0):.0f}/100",
                f"Fresh: {pick['change_7d']:+.1f}%",
                f"{pick['buzz_level']} ({pick['twitter_mentions']}üê¶ {pick['reddit_mentions']}ü§ñ)",
                f"{pick['cap_size']}",
                f"Short: {pick['short_percent']:.1f}%"
            ]
            
            embed.add_embed_field(
                name=f"#{i}. {pick['ticker']} {signal_str}",
                value=" | ".join(value_parts),
                inline=False
            )
        
        if len(picks) > 10:
            embed.add_embed_field(
                name="‚ûï More Picks",
                value=f"Plus {len(picks)-10} more (see CSV)",
                inline=False
            )
        
        embed.add_embed_field(
            name="üìä Summary",
            value=f"Total: {len(picks)} | Validated: 70% WR, +4.71% avg (37 trades) | Alpha: +8.96 pts vs S&P",
            inline=False
        )
        
        embed.set_footer(text=f"V3 Validated Edge | {datetime.now().strftime('%Y-%m-%d %I:%M %p')}")
        
        webhook.add_embed(embed)
        response = webhook.execute()
        print(f"‚úÖ Discord sent ({len(picks)} picks)")
    
    except Exception as e:
        print(f"‚ùå Discord failed: {e}")


if __name__ == "__main__":
    start_time = datetime.now()
    
    picks = scan()
    
    elapsed = (datetime.now() - start_time).total_seconds()
    
    display_picks(picks)
    
    if picks:
        save_picks(picks)
    
    print(f"\n{'='*70}")
    print("üì§ SENDING DISCORD NOTIFICATION...")
    print(f"{'='*70}")
    send_discord_notification(picks)
    print(f"{'='*70}\n")
    
    print(f"\n‚è±Ô∏è  Scan completed in {elapsed:.1f} seconds")
    
    if picks:
        print(f"‚úÖ V3 Scan Complete!\n")
    else:
        print(f"‚ùå No picks today\n")